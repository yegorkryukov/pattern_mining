{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a **contiguous sequential pattern mining** algorithm and apply it on text data to mine potential phrase candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_word_freq(documents):\n",
    "    \"\"\"Calculates the frequency of each word in the input document.\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    word_freq = Counter()\n",
    "    active_indices = []\n",
    "    for doc_index, doc in enumerate(documents):\n",
    "        words = doc.split(' ')\n",
    "        word_indices = []\n",
    "        for word_index, word in enumerate(words):\n",
    "            word_freq[word] += 1\n",
    "            word_indices.append(word_index)\n",
    "            total_words += 1\n",
    "        active_indices.append(word_indices)\n",
    "\n",
    "    return total_words, word_freq, active_indices\n",
    "\n",
    "def frequent_pattern_mining(documents, min_support, max_phrase_size, word_freq, active_indices):\n",
    "    \"\"\"\n",
    "    Performs frequent pattern mining to collect aggregate counts for all contiguous phrases in the \n",
    "    input document that satisfy a certain minimum support threshold.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    documents : list(str)\n",
    "        the input corpus\n",
    "    min_support : int\n",
    "        minimum support threshold which must be satisfied by each phrase.\n",
    "    max_phrase_size : int\n",
    "        maximum allowed phrase size\n",
    "    word_freq : dict-like\n",
    "        raw frequency of each word in the input corpus\n",
    "    active_indices : list(list(int))\n",
    "        set of active indices\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    \n",
    "    \"\"\" \n",
    "    hash_counter = word_freq.copy()\n",
    "    n = 2\n",
    "\n",
    "    #iterate until documents is empty\n",
    "    while(len(documents) > 0):\n",
    "        temp_documents = []\n",
    "        new_active_indices = []\n",
    "        #go over each document\n",
    "        for d_i,doc in enumerate(documents):\n",
    "            #get set of indices of phrases of length n-1 with min support\n",
    "            new_word_indices = []\n",
    "            word_indices = active_indices[d_i]\n",
    "            for index in word_indices:\n",
    "                words = doc.split()\n",
    "                if index+n-2 < len(words):\n",
    "                    key = \"\"\n",
    "                    for i in range(index, index+n-2+1):\n",
    "                        if i == index+n-2:\n",
    "                            key = key + words[i]\n",
    "                        else:\n",
    "                            key = key + words[i] + \" \"\n",
    "                    \n",
    "                    #check if the phrase 'key' meets min support\n",
    "                    if (hash_counter[key] >= min_support):\n",
    "                        new_word_indices.append(index)\n",
    "\n",
    "            #remove the current document if there is no more phrases of length\n",
    "            #n which satisfy the minimum support threshold\n",
    "            if len(new_word_indices) != 0:\n",
    "                new_active_indices.append(new_word_indices)\n",
    "                temp_documents.append(doc)\n",
    "                words = doc.split()\n",
    "                for idx, i in enumerate(new_word_indices[:-1]):\n",
    "                    phrase = \"\"\n",
    "                    if (new_word_indices[idx+1] == i + 1):\n",
    "                        for idx in range(i, i+n):\n",
    "                            if idx == i+n-1:\n",
    "                                phrase += words[idx]\n",
    "                            else:\n",
    "                                phrase += words[idx] + \" \"\n",
    "                    hash_counter[phrase] += 1\n",
    "\n",
    "        documents = temp_documents\n",
    "        active_indices = new_active_indices\n",
    "        n += 1\n",
    "        if n == max_phrase_size:\n",
    "            break\n",
    "\n",
    "    hash_counter = Counter(x for x in hash_counter.elements() if hash_counter[x] >= min_support)\n",
    "\n",
    "    return hash_counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hoagie institution walking doe seem like throwback year ago old fashioned menu board booth large selection food speciality italian hoagie voted best area year year usually order burger patty obviously cooked frozen ingredient fresh overall good alternative subway road',\n",
       " 'excellent food superb customer service miss mario machine used still great place steeped tradition',\n",
       " 'yes place little dated opened weekend staff always pleasant fast make order always spot fresh veggie hoggies food also daily special ice cream really good banana split piled topping win pennysaver award ever year see',\n",
       " 'food great best thing wing wing simply fantastic wet cajun best popular also like seasoned salt wing wing night monday wednesday night whole wing dining area nice family friendly bar nice well place truly yinzer dream pittsburgh dad would love place',\n",
       " 'checked place past monday wing night heard wing great decided finally time check wing whole wing crispy nice change pace got wet cajun sauce garlic butter wing cajun bold enough flavor sauce thin sauce also thin garlic butter expected better average like seeing sauce resting bottom boat would definitely come try place sample item menu probably become regular stop wing anytime soon']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('reviews_sample.txt', 'r') as f:\n",
    "    data = f.read().split('\\n')\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = data\n",
    "min_support = 100\n",
    "max_phrase_size = 5\n",
    "\n",
    "total_words, word_freq, active_indices = get_word_freq(docs)\n",
    "\n",
    "vocab_size = len(word_freq)\n",
    "\n",
    "hash_counter = frequent_pattern_mining(docs, min_support, max_phrase_size, word_freq, active_indices)\n",
    "hash_counter.pop('')\n",
    "len(hash_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patterns.txt', 'w') as f:\n",
    "    f.write(\n",
    "        '\\n'.join([f'{v}:{\";\".join(k.split())}' for (k, v) in\n",
    "            sorted(hash_counter.items(), key=lambda item: item[1], reverse=True)])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
